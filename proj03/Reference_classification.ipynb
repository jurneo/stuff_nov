{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference classification with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the assignment for the DL course part 3 of using LSTM model for solving text classification.\n",
    "\n",
    "The input is the reference papers database which I have manually label and categorize.\n",
    "\n",
    "The papers format are in PDF files -- some preprocessing manual and semi-automatic is involved.\n",
    "\n",
    "The amount of input is not many, I start first with a small amount of dataset.\n",
    "\n",
    "Update 13/11: two categories at moment\n",
    "\n",
    "There are about 80 documents for each categories with one document is about 5000 words.\n",
    "\n",
    "For the purpose of testing, I limit first to 1000 words\n",
    "\n",
    "Reference for this code is from the course as well as [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\k\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\k\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "import keras\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from pandas import DataFrame\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "max_features = 500\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Limit each newsline to a fixed number of words\n",
    "document_max_num_words = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs1 = pickle.load(open('cnn.pkl','rb'))\n",
    "docs2 = pickle.load(open('device_characterization.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "for d1 in docs1:\n",
    "    docs.append(d1)\n",
    "for d2 in docs2:\n",
    "    docs.append(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words in corpus: 168\n",
      "{'device characterizations': 0, 'convolutional neural network': 1}\n"
     ]
    }
   ],
   "source": [
    "alltext = []\n",
    "key_label = set()\n",
    "for p1, p2 in docs:\n",
    "    alltext.append(p2) # = alltext + p2\n",
    "    key_label.add(p1) \n",
    "i = 0\n",
    "label_map = {}\n",
    "for k in key_label:\n",
    "    label_map[k] = i\n",
    "    i = i + 1\n",
    "print('total words in corpus:',len(alltext))\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(alltext)\n",
    "num_features = 500\n",
    "w2v_model = Word2Vec(alltext, size=num_features, min_count=1, window=10)\n",
    "w2v_model.init_sims(replace=True)\n",
    "w2v_model.save('test.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_documents = len(docs)\n",
    "number_of_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_categories = 2\n",
    "X = zeros(shape=(number_of_documents, document_max_num_words, num_features)).astype(float32)\n",
    "Y = zeros(shape=(number_of_documents, num_categories)).astype(float32)\n",
    "empty_word = zeros(num_features).astype(float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx, document in enumerate(alltext):\n",
    "    for jdx, word in enumerate(document):\n",
    "        if jdx == document_max_num_words:\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            if word in w2v_model:\n",
    "                X[idx, jdx, :] = w2v_model[word]\n",
    "            else:\n",
    "                X[idx, jdx, :] = empty_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx, key in enumerate(label_map.keys()):\n",
    "    Y[idx, :] = label_map[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(int(document_max_num_words*1.5), input_shape=(document_max_num_words, num_features)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_categories))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k\\Anaconda3\\lib\\site-packages\\keras\\models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 117 samples, validate on 51 samples\n",
      "Epoch 1/5\n",
      "117/117 [==============================] - 0s - loss: 0.2694 - acc: 0.9915 - val_loss: 0.1271 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "117/117 [==============================] - 0s - loss: 0.1475 - acc: 0.9915 - val_loss: 0.0403 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "117/117 [==============================] - 0s - loss: 0.0682 - acc: 0.9915 - val_loss: 0.0137 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "117/117 [==============================] - 0s - loss: 0.0480 - acc: 0.9915 - val_loss: 0.0064 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "117/117 [==============================] - 0s - loss: 0.0524 - acc: 0.9915 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "51/51 [==============================] - 0s\n",
      "Score: 0.0038\n",
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit(X_train, Y_train, batch_size=128, nb_epoch=5, validation_data=(X_test, Y_test))\n",
    "\n",
    "# Evaluate model\n",
    "score, acc = model.evaluate(X_test, Y_test, batch_size=128)\n",
    "    \n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vdocs = []\n",
    "# for i in range(0, len(docs)):\n",
    "#     k = label_map[ docs[i][0] ]\n",
    "#     v = tokenizer.texts_to_sequences(docs[i][1]) # the result is list of list\n",
    "#     # we need to make it list of values\n",
    "#     vv = [q[0] for q in v if q != []]\n",
    "#     vdocs.append((k,vv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random.seed(1000)\n",
    "# random.shuffle(vdocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_split = 0.2\n",
    "# pad = 0\n",
    "\n",
    "# # train size\n",
    "# sz1 = int(len(vdocs)*(1-test_split))\n",
    "# X_train1 = []\n",
    "# y_train1 = []\n",
    "# for i in range(0, sz1):\n",
    "#     label = vdocs[i][0]\n",
    "#     y_train1.append(label)\n",
    "#     txt1 = vdocs[i][1]\n",
    "#     if len(txt1)<max_words:\n",
    "#         for j in range(len(txt1), max_words+1):\n",
    "#             txt1.append(pad)\n",
    "#     else:\n",
    "#         txt1 = txt1[:max_words]\n",
    "#     X_train1.append(txt1)\n",
    "\n",
    "# X_test1 = []\n",
    "# y_test1 = []\n",
    "# for i in range(sz1, len(vdocs)):\n",
    "#     label = vdocs[i][0]\n",
    "#     y_test1.append(label)\n",
    "#     txt1 = vdocs[i][1]\n",
    "#     if len(txt1)<max_words:\n",
    "#         for j in range(len(txt1), max_words+1):\n",
    "#             txt1.append(pad)\n",
    "#     else:\n",
    "#         txt1 = txt1[:max_words]\n",
    "#     X_test1.append(txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train = np.array(X_train1)\n",
    "# y_train = np.array(y_train1)\n",
    "# X_test = np.array(X_test1)\n",
    "# y_test = np.array(y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# num_features=500\n",
    "# num_categories=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# print('Build model...')\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(max_features, 128))\n",
    "# model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # try using different optimizers and different optimizer configs\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print('Train...')\n",
    "# model.fit(X_train, y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=15,\n",
    "#           validation_data=(X_test, y_test)\n",
    "#          )\n",
    "# score, acc = model.evaluate(X_test, y_test,\n",
    "#                             batch_size=batch_size)\n",
    "# print('Test score:', score)\n",
    "# print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
